<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
   
  <meta name="keywords" content="广外编程社" />
   
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Python 爬虫（二） |  广外编程社官网
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/main.css">

  
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
  
  

  

<link rel="alternate" href="/atom.xml" title="广外编程社官网" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-Python-爬虫（二）" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Python 爬虫（二）
</h1>
  

    </header>
    

    
    <div class="article-meta">
      
        <!-- 不蒜子统计 -->
        <span id="busuanzi_container_page_pv" style='display:none' class="article-date">
              <i class="icon-smile icon"></i> 阅读数：<span id="busuanzi_value_page_pv"></span>次
        </span>


<a href="/2020/06/13/Python-%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-date">
  <time datetime="2020-06-13T11:59:47.000Z" itemprop="datePublished">2020-06-13</time>
</a>
      
      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">8.8k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">36分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p> Github代码获取：<a href="https://github.com/Jack-Cherish/python-spider" target="_blank" rel="noopener">https://github.com/Jack-Cherish/python-spider</a><br>Python版本： Python3.x<br>运行平台： Windows<br>IDE： Sublime text3</p>
<hr>
<h2 id="更多教程，请查看：https-cuijiahua-com-blog-spider"><a href="#更多教程，请查看：https-cuijiahua-com-blog-spider" class="headerlink" title="更多教程，请查看：https://cuijiahua.com/blog/spider/"></a>更多教程，请查看：<a href="https://cuijiahua.com/blog/spider/" target="_blank" rel="noopener">https://cuijiahua.com/blog/spider/</a></h2><h1 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h1><p><strong>强烈建议：</strong>请在电脑的陪同下，阅读本文。本文以实战为主，阅读过程如稍有不适，还望多加练习。<br>本文的实战内容有：</p>
<ul>
<li>网络小说下载(静态网站)</li>
<li>优美壁纸下载(动态网站)</li>
<li>视频下载</li>
</ul>
<hr>
<p>2020年，更多精彩内容，尽在微信公众号，欢迎您的关注：</p>
<p><img src="https://img-blog.csdnimg.cn/20200423085559318.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2M0MDY0OTU3NjI=,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="二-网络爬虫简介"><a href="#二-网络爬虫简介" class="headerlink" title="二 网络爬虫简介"></a>二 网络爬虫简介</h1><p>网络爬虫，也叫网络蜘蛛(Web Spider)。它根据网页地址(URL)爬取网页内容，而网页地址(URL)就是我们在浏览器中输入的网站链接。比如：<a href="https://www.baidu.com/，它就是一个URL。" target="_blank" rel="noopener">https://www.baidu.com/，它就是一个URL。</a></p>
<p>在讲解爬虫内容之前，我们需要先学习一项写爬虫的必备技能：<strong>审查元素（如果已掌握，可跳过此部分内容）。</strong></p>
<h2 id="1-审查元素"><a href="#1-审查元素" class="headerlink" title="1 审查元素"></a>1 审查元素</h2><p>在浏览器的地址栏输入URL地址，在网页处右键单击，找到检查。(不同浏览器的叫法不同，Chrome浏览器叫做检查，Firefox浏览器叫做查看元素，但是功能都是相同的)</p>
<p><img src="https://img-blog.csdn.net/20170928142328881?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们可以看到，右侧出现了一大推代码，这些代码就叫做HTML。什么是HTML？举个容易理解的例子：<strong>我们的基因决定了我们的原始容貌，服务器返回的HTML决定了网站的原始容貌。</strong></p>
<p><img src="https://img-blog.csdn.net/20170928142436698?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>为啥说是<strong>原始容貌</strong>呢？因为人可以<strong>整容啊</strong>！扎心了，有木有？<strong>那网站也可以”整容”吗？可以！请看下图：</strong></p>
<p><img src="https://img-blog.csdn.net/20170928142523847?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我能有这么多钱吗？显然不可能。我是怎么给网站”整容”的呢？就是通过修改服务器返回的HTML信息。我们每个人都是”整容大师”，可以修改页面信息。<strong>我们在页面的哪个位置点击审查元素，浏览器就会为我们定位到相应的HTML位置，进而就可以在本地更改HTML信息。</strong></p>
<p><strong>再举个小例子：</strong>我们都知道，使用浏览器”记住密码”的功能，密码会变成一堆小黑点，是不可见的。可以让密码显示出来吗？可以，只需给页面”动个小手术”！以淘宝为例，在输入密码框处右键，点击检查。</p>
<p><img src="https://img-blog.csdn.net/20170928142556264?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>可以看到，浏览器为我们自动定位到了相应的HTML位置。将下图中的password属性值改为text属性值(<strong>直接在右侧代码处修改</strong>)：</p>
<p><img src="https://img-blog.csdn.net/20170928142703187?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们让浏览器记住的密码就这样显现出来了：</p>
<p><img src="https://img-blog.csdn.net/20170928142630494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>说这么多，什么意思呢？<strong>浏览器就是作为客户端从服务器端获取信息，然后将信息解析，并展示给我们的。</strong>我们可以在本地修改HTML信息，为网页”整容”，但是我们修改的信息不会回传到服务器，服务器存储的HTML信息不会改变。刷新一下界面，页面还会回到原本的样子。<strong>这就跟人整容一样，我们能改变一些表面的东西，但是不能改变我们的基因。</strong></p>
<h2 id="2-简单实例"><a href="#2-简单实例" class="headerlink" title="2 简单实例"></a>2 简单实例</h2><p><strong>网络爬虫的第一步就是根据URL，获取网页的HTML信息。\</strong>在Python3中，可以使用*<em>urllib.request*</em>和<strong>requests</strong>进行网页爬取。</p>
<ul>
<li>urllib库是python内置的，无需我们额外安装，只要安装了Python就可以使用这个库。</li>
<li>requests库是第三方库，需要我们自己安装。</li>
</ul>
<p>requests库强大好用，所以本文使用requests库获取网页的HTML信息。requests库的github地址：<a href="https://github.com/requests/requests" target="_blank" rel="noopener">https://github.com/requests/requests</a></p>
<h3 id="1-requests安装"><a href="#1-requests安装" class="headerlink" title="(1) requests安装"></a>(1) requests安装</h3><p>在cmd中，使用如下指令安装requests：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>或者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">easy_install requests</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<h3 id="2-简单实例-1"><a href="#2-简单实例-1" class="headerlink" title="(2) 简单实例"></a>(2) 简单实例</h3><p>requests库的基础方法如下：</p>
<p><img src="https://img-blog.csdn.net/20170928142920593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>官方中文教程地址：<a href="http://docs.python-requests.org/zh_CN/latest/user/quickstart.html" target="_blank" rel="noopener">http://docs.python-requests.org/zh_CN/latest/user/quickstart.html</a></p>
<p>requests库的开发者为我们提供了详细的中文教程，查询起来很方便。本文不会对其所有内容进行讲解，摘取其部分使用到的内容，进行实战说明。</p>
<p>首先，让我们看下requests.get()方法，它用于向服务器发起GET请求，不了解GET请求没有关系。我们可以这样理解：get的中文意思是得到、抓住，那这个requests.get()方法就是从服务器得到、抓住数据，也就是获取数据。让我们看一个例子(以 <a href="http://www.gitbook.cn为例)来加深理解：">www.gitbook.cn为例)来加深理解：</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    target &#x3D; &#39;http:&#x2F;&#x2F;gitbook.cn&#x2F;&#39;</span><br><span class="line">    req &#x3D; requests.get(url&#x3D;target)</span><br><span class="line">    print(req.text)</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>

<p>requests.get()方法必须设置的一个参数就是url，因为我们得告诉GET请求，我们的目标是谁，我们要获取谁的信息。运行程序看下结果：</p>
<p><img src="https://img-blog.csdn.net/20170928143016033?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>左侧是我们程序获得的结果，右侧是我们在<a href="http://www.gitbook.cn网站审查元素获得的信息。我们可以看到，我们已经顺利获得了该网页的HTML信息。这就是一个最简单的爬虫实例，可能你会问，我只是爬取了这个网页的HTML信息，有什么用呢？客官稍安勿躁，接下来进入我们的实战正文。">www.gitbook.cn网站审查元素获得的信息。我们可以看到，我们已经顺利获得了该网页的HTML信息。这就是一个最简单的爬虫实例，可能你会问，我只是爬取了这个网页的HTML信息，有什么用呢？客官稍安勿躁，接下来进入我们的实战正文。</a></p>
<hr>
<h1 id="三-爬虫实战"><a href="#三-爬虫实战" class="headerlink" title="三 爬虫实战"></a>三 爬虫实战</h1><h2 id="1-小说下载"><a href="#1-小说下载" class="headerlink" title="1 小说下载"></a>1 小说下载</h2><h3 id="1-实战背景"><a href="#1-实战背景" class="headerlink" title="(1) 实战背景"></a>(1) 实战背景</h3><p>小说网站-笔趣看：URL：<a href="http://www.biqukan.com/" target="_blank" rel="noopener">http://www.biqukan.com/</a></p>
<p>本次实战就是从该网站爬取并保存一本名为《一念永恒》的小说。</p>
<h3 id="2-小试牛刀"><a href="#2-小试牛刀" class="headerlink" title="(2) 小试牛刀"></a>(2) 小试牛刀</h3><p>我们先看下《一念永恒》小说的第一章内容，URL：<a href="http://www.biqukan.com/1_1094/5403177.html" target="_blank" rel="noopener">http://www.biqukan.com/1_1094/5403177.html</a></p>
<p><img src="https://img-blog.csdn.net/20170928143125108?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们先用已经学到的知识获取HTML信息试一试，编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    target &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;5403177.html&#39;</span><br><span class="line">    req &#x3D; requests.get(url&#x3D;target)</span><br><span class="line">    print(req.text)</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>

<p>运行代码，可以看到如下结果：</p>
<p><img src="https://img-blog.csdn.net/20170928143206652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>可以看到，我们很轻松地获取了HTML信息。但是，很显然，很多信息是我们不想看到的，我们只想获得如右侧所示的正文内容，我们不关心div、br这些html标签。<strong>如何把正文内容从这些众多的html标签中提取出来呢？这就是本次实战的主要内容。</strong></p>
<p>###（3）Beautiful Soup</p>
<p><strong>爬虫的第一步，获取整个网页的HTML信息，我们已经完成。接下来就是爬虫的第二步，解析HTML信息，提取我们感兴趣的内容。</strong>对于本小节的实战，我们感兴趣的内容就是文章的正文。提取的方法有很多，例如使用正则表达式、Xpath、Beautiful Soup等。对于初学者而言，最容易理解，并且使用简单的方法就是使用Beautiful Soup提取感兴趣内容。</p>
<p>Beautiful Soup的安装方法和requests一样，使用如下指令安装(也是二选一)：</p>
<ul>
<li>pip install beautifulsoup4</li>
<li>easy_install beautifulsoup4</li>
</ul>
<p>一个强大的第三方库，都会有一个详细的官方文档。我们很幸运，Beautiful Soup也是有中文的官方文档。</p>
<p>URL：<a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">http://beautifulsoup.readthedocs.io/zh_CN/latest/</a></p>
<p>同理，我会根据实战需求，讲解Beautiful Soup库的部分使用方法，更详细的内容，请查看官方文档。</p>
<p>现在，我们使用已经掌握的审查元素方法，查看一下我们的目标页面，你会看到如下内容：</p>
<p><img src="https://img-blog.csdn.net/20170928143325915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>不难发现，文章的所有内容都放在了一个名为div的“东西下面”，这个”东西”就是html标签。HTML标签是HTML语言中最基本的单位，HTML标签是HTML最重要的组成部分。不理解，没关系，我们再举个简单的例子：</p>
<blockquote>
<p>一个女人的包包里，会有很多东西，她们会根据自己的习惯将自己的东西进行分类放好。镜子和口红这些会经常用到的东西，会归放到容易拿到的外侧口袋里。那些不经常用到，需要注意安全存放的证件会放到不容易拿到的里侧口袋里。</p>
</blockquote>
<p>html标签就像一个个“口袋”，每个“口袋”都有自己的特定功能，负责存放不同的内容。显然，上述例子中的div标签下存放了我们关心的正文内容。这个div标签是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id&#x3D;&quot;content&quot;, class&#x3D;&quot;showtxt&quot;&gt;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>细心的朋友可能已经发现，除了div字样外，还有id和class。id和class就是div标签的属性，content和showtxt是属性值，一个属性对应一个属性值。这东西有什么用？它是用来区分不同的div标签的，因为div标签可以有很多，我们怎么加以区分不同的div标签呢？就是通过不同的属性值。</p>
<p>仔细观察目标网站一番，我们会发现这样一个事实：<strong>class属性为showtxt的div标签，独一份！这个标签里面存放的内容，是我们关心的正文部分。</strong></p>
<p>知道这个信息，我们就可以使用Beautiful Soup提取我们想要的内容了，编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;5403177.html&#39;</span><br><span class="line">     req &#x3D; requests.get(url &#x3D; target)</span><br><span class="line">     html &#x3D; req.text</span><br><span class="line">     bf &#x3D; BeautifulSoup(html)</span><br><span class="line">     texts &#x3D; bf.find_all(&#39;div&#39;, class_ &#x3D; &#39;showtxt&#39;) print(texts)</span><br><span class="line">123456789</span><br></pre></td></tr></table></figure>

<p>在解析html之前，我们需要创建一个Beautiful Soup对象。BeautifulSoup函数里的参数就是我们已经获得的html信息。然后我们使用find_all方法，获得html信息中所有class属性为showtxt的div标签。find_all方法的第一个参数是获取的标签名，第二个参数class_是标签的属性，为什么不是class，而带了一个下划线呢？因为python中class是关键字，为了防止冲突，这里使用class_表示标签的class属性，class_后面跟着的showtxt就是属性值了。看下我们要匹配的标签格式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id&#x3D;&quot;content&quot;, class&#x3D;&quot;showtxt&quot;&gt;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>这样对应的看一下，是不是就懂了？可能有人会问了，为什么不是find_all(‘div’, id = ‘content’, class_ = ‘showtxt’)?这样其实也是可以的，属性是作为查询时候的约束条件，添加一个class_=’showtxt’条件，我们就已经能够准确匹配到我们想要的标签了，所以我们就不必再添加id这个属性了。运行代码查看我们匹配的结果：</p>
<p><img src="https://img-blog.csdn.net/20170928143457737?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们可以看到，我们已经顺利匹配到我们关心的正文内容，但是还有一些我们不想要的东西。比如div标签名，br标签，以及各种空格。怎么去除这些东西呢？我们继续编写代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;5403177.html&#39;</span><br><span class="line">     req &#x3D; requests.get(url &#x3D; target) html &#x3D; req.text</span><br><span class="line">     bf &#x3D; BeautifulSoup(html)</span><br><span class="line">     texts &#x3D; bf.find_all(&#39;div&#39;, class_ &#x3D; &#39;showtxt&#39;)</span><br><span class="line">     print(texts[0].text.replace(&#39;\xa0&#39;*8,&#39;\n\n&#39;))</span><br><span class="line">123456789</span><br></pre></td></tr></table></figure>

<p>find_all匹配的返回的结果是一个列表。提取匹配结果后，使用text属性，提取文本内容，滤除br标签。随后使用replace方法，剔除空格，替换为回车进行分段。 在html中是用来表示空格的。replace(’\xa0’*8,’\n\n’)就是去掉下图的八个空格符号，并用回车代替：</p>
<p><img src="https://img-blog.csdn.net/20170928143543366?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>程序运行结果如下：</p>
<p><img src="https://img-blog.csdn.net/20170928143634255?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>可以看到，我们很自然的匹配到了所有正文内容，并进行了分段。我们已经顺利获得了一个章节的内容，要想下载正本小说，我们就要获取每个章节的链接。我们先分析下小说目录：</p>
<p>URL：<a href="http://www.biqukan.com/1_1094/" target="_blank" rel="noopener">http://www.biqukan.com/1_1094/</a></p>
<p><img src="https://img-blog.csdn.net/20170928143709074?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>通过审查元素，我们发现可以发现，这些章节都存放在了class属性为listmain的div标签下，选取部分html代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class&#x3D;&quot;listmain&quot;&gt;</span><br><span class="line">&lt;dl&gt;</span><br><span class="line">&lt;dt&gt;《一念永恒》最新章节列表&lt;&#x2F;dt&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15932394.html&quot;&gt;第1027章 第十道门&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15923072.html&quot;&gt;第1026章 绝伦道法！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15921862.html&quot;&gt;第1025章 长生灯！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15918591.html&quot;&gt;第1024章 一目晶渊&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15906236.html&quot;&gt;第1023章 通天道门&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15903775.html&quot;&gt;第1022章 四大凶兽！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15890427.html&quot;&gt;第1021章 鳄首！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15886627.html&quot;&gt;第1020章 一触即发！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15875306.html&quot;&gt;第1019章 魁祖的气息！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15871572.html&quot;&gt;第1018章 绝望的魁皇城&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15859514.html&quot;&gt;第1017章 我还是恨你！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;15856137.html&quot;&gt;第1016章 从来没有世界之门！&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dt&gt;《一念永恒》正文卷&lt;&#x2F;dt&gt; &lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5386269.html&quot;&gt;外传1 柯父。&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5386270.html&quot;&gt;外传2 楚玉嫣。&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt; &lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5386271.html&quot;&gt;外传3 鹦鹉与皮冻。&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5403177.html&quot;&gt;第一章 他叫白小纯&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt; &lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5428081.html&quot;&gt;第二章 火灶房&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5433843.html&quot;&gt;第三章 六句真言&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt; &lt;dd&gt;&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5447905.html&quot;&gt;第四章 炼灵&lt;&#x2F;a&gt;&lt;&#x2F;dd&gt;</span><br><span class="line">&lt;&#x2F;dl&gt;</span><br><span class="line">&lt;&#x2F;div&gt;</span><br><span class="line">123456789101112131415161718192021</span><br></pre></td></tr></table></figure>

<p>在分析之前，让我们先介绍一个概念：父节点、子节点、孙节点。<code>&lt;div&gt;</code>和<code>&lt;/div&gt;</code>限定了<code>&lt;div&gt;</code>标签的开始和结束的位置，他们是成对出现的，有开始位置，就有结束位置。我们可以看到，在<code>&lt;div&gt;</code>标签包含<code>&lt;dl&gt;</code>标签，那这个<code>&lt;dl&gt;</code>标签就是<code>&lt;div&gt;</code>标签的子节点，<code>&lt;dl&gt;</code>标签又包含<code>&lt;dt&gt;</code>标签和<code>&lt;dd&gt;</code>标签，那么<code>&lt;dt&gt;</code>标签和<code>&lt;dd&gt;</code>标签就是<code>&lt;div&gt;</code>标签的孙节点。有点绕？那你记住这句话：<strong>谁包含谁，谁就是谁儿子！</strong></p>
<p><strong>他们之间的关系都是相对的。</strong>比如对于<code>&lt;dd&gt;</code>标签，它的子节点是<code>&lt;a&gt;</code>标签，它的父节点是<code>&lt;dl&gt;</code>标签。这跟我们人是一样的，上有老下有小。</p>
<p>看到这里可能有人会问，这有好多<code>&lt;dd&gt;</code>标签和<code>&lt;a&gt;</code>标签啊！不同的<code>&lt;dd&gt;</code>标签，它们是什么关系啊？显然，兄弟姐妹喽！我们称它们为兄弟结点。<br>好了，概念明确清楚，接下来，让我们分析一下问题。我们看到每个章节的名字存放在了<code>&lt;a&gt;</code>标签里面。<code>&lt;a&gt;</code>标签还有一个href属性。这里就不得不提一下<code>&lt;a&gt;</code>标签的定义了，<code>&lt;a&gt;</code>标签定义了一个超链接，用于从一张页面链接到另一张页面。<code>&lt;a&gt;</code> 标签最重要的属性是 href 属性，它指示链接的目标。</p>
<p>我们将之前获得的第一章节的URL和<code>&lt;a&gt;</code> 标签对比看一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;5403177.html</span><br><span class="line">&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5403177.html&quot;&gt;第一章 他叫白小纯&lt;&#x2F;a&gt;</span><br><span class="line">12</span><br></pre></td></tr></table></figure>

<p>不难发现，<code>&lt;a&gt;</code> 标签中href属性存放的属性值/1_1094/5403177.html是章节URL<a href="http://www.biqukan.com/1_1094/5403177.html的后半部分。其他章节也是如此！那这样，我们就可以根据`" target="_blank" rel="noopener">http://www.biqukan.com/1_1094/5403177.html的后半部分。其他章节也是如此！那这样，我们就可以根据`</a><a>`标签的href属性值获得每个章节的链接和名称了。</p>
<p>总结一下：小说每章的链接放在了class属性为listmain的<code>&lt;div&gt;</code>标签下的<code>&lt;a&gt;</code>标签中。链接具体位置放在html-&gt;body-&gt;div-&gt;dl-&gt;dd-&gt;a的href属性中。先匹配class属性为listmain的<code>&lt;div&gt;</code>标签，再匹配<code>&lt;a&gt;</code>标签。编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;&#39;</span><br><span class="line">     req &#x3D; requests.get(url &#x3D; target)</span><br><span class="line">     html &#x3D; req.text</span><br><span class="line">     div_bf &#x3D; BeautifulSoup(html)</span><br><span class="line">     div &#x3D; div_bf.find_all(&#39;div&#39;, class_ &#x3D; &#39;listmain&#39;)</span><br><span class="line">     print(div[0])</span><br><span class="line">12345678910</span><br></pre></td></tr></table></figure>

<p>还是使用find_all方法，运行结果如下：</p>
<p><img src="https://img-blog.csdn.net/20170928144048324?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>很顺利，接下来再匹配每一个<code>&lt;a&gt;</code>标签，并提取章节名和章节文章。如果我们使用Beautiful Soup匹配到了下面这个<code>&lt;a&gt;</code>标签，如何提取它的href属性和<code>&lt;a&gt;</code>标签里存放的章节名呢？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href&#x3D;&quot;&#x2F;1_1094&#x2F;5403177.html&quot;&gt;第一章 他叫白小纯&lt;&#x2F;a&gt;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>方法很简单，对Beautiful Soup返回的匹配结果a，使用a.get(‘href’)方法就能获取href的属性值，使用a.string就能获取章节名，编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">     server &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;&#39;</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;&#39;</span><br><span class="line">     req &#x3D; requests.get(url &#x3D; target) html &#x3D; req.text</span><br><span class="line">     div_bf &#x3D; BeautifulSoup(html)</span><br><span class="line">     div &#x3D; div_bf.find_all(&#39;div&#39;, class_ &#x3D; &#39;listmain&#39;)</span><br><span class="line">     a_bf &#x3D; BeautifulSoup(str(div[0]))</span><br><span class="line">     a &#x3D; a_bf.find_all(&#39;a&#39;)</span><br><span class="line">     for each in a:</span><br><span class="line">          print(each.string, server + each.get(&#39;href&#39;))</span><br><span class="line">12345678910111213</span><br></pre></td></tr></table></figure>

<p>因为find_all返回的是一个列表，里边存放了很多的<code>&lt;a&gt;</code>标签，所以使用for循环遍历每个<code>&lt;a&gt;</code>标签并打印出来，运行结果如下。</p>
<p><img src="https://img-blog.csdn.net/20170928144215561?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>最上面匹配的一千多章的内容是最新更新的12章节的链接。这12章内容会和下面的重复，所以我们要滤除，除此之外，还有那3个外传，我们也不想要。这些都简单地剔除就好。</p>
<p>###（3）整合代码</p>
<p>每个章节的链接、章节名、章节内容都有了。接下来就是整合代码，将获得内容写入文本文件存储就好了。编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import requests, sys</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">类说明:下载《笔趣看》网小说《一念永恒》</span><br><span class="line">Parameters:</span><br><span class="line">    无</span><br><span class="line">Returns:</span><br><span class="line">    无</span><br><span class="line">Modify:</span><br><span class="line">    2017-09-13</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">class downloader(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.server &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;&#39;</span><br><span class="line">        self.target &#x3D; &#39;http:&#x2F;&#x2F;www.biqukan.com&#x2F;1_1094&#x2F;&#39;</span><br><span class="line">        self.names &#x3D; []            #存放章节名</span><br><span class="line">        self.urls &#x3D; []            #存放章节链接</span><br><span class="line">        self.nums &#x3D; 0            #章节数</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    函数说明:获取下载链接</span><br><span class="line">    Parameters:</span><br><span class="line">        无</span><br><span class="line">    Returns:</span><br><span class="line">        无</span><br><span class="line">    Modify:</span><br><span class="line">        2017-09-13</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def get_download_url(self):</span><br><span class="line">        req &#x3D; requests.get(url &#x3D; self.target)</span><br><span class="line">        html &#x3D; req.text</span><br><span class="line">        div_bf &#x3D; BeautifulSoup(html)</span><br><span class="line">        div &#x3D; div_bf.find_all(&#39;div&#39;, class_ &#x3D; &#39;listmain&#39;)</span><br><span class="line">        a_bf &#x3D; BeautifulSoup(str(div[0]))</span><br><span class="line">        a &#x3D; a_bf.find_all(&#39;a&#39;)</span><br><span class="line">        self.nums &#x3D; len(a[15:])                                #剔除不必要的章节，并统计章节数</span><br><span class="line">        for each in a[15:]:</span><br><span class="line">            self.names.append(each.string)</span><br><span class="line">            self.urls.append(self.server + each.get(&#39;href&#39;))</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    函数说明:获取章节内容</span><br><span class="line">    Parameters:</span><br><span class="line">        target - 下载连接(string)</span><br><span class="line">    Returns:</span><br><span class="line">        texts - 章节内容(string)</span><br><span class="line">    Modify:</span><br><span class="line">        2017-09-13</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def get_contents(self, target):</span><br><span class="line">        req &#x3D; requests.get(url &#x3D; target)</span><br><span class="line">        html &#x3D; req.text</span><br><span class="line">        bf &#x3D; BeautifulSoup(html)</span><br><span class="line">        texts &#x3D; bf.find_all(&#39;div&#39;, class_ &#x3D; &#39;showtxt&#39;)</span><br><span class="line">        texts &#x3D; texts[0].text.replace(&#39;\xa0&#39;*8,&#39;\n\n&#39;)</span><br><span class="line">        return texts</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    函数说明:将爬取的文章内容写入文件</span><br><span class="line">    Parameters:</span><br><span class="line">        name - 章节名称(string)</span><br><span class="line">        path - 当前路径下,小说保存名称(string)</span><br><span class="line">        text - 章节内容(string)</span><br><span class="line">    Returns:</span><br><span class="line">        无</span><br><span class="line">    Modify:</span><br><span class="line">        2017-09-13</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def writer(self, name, path, text):</span><br><span class="line">        write_flag &#x3D; True</span><br><span class="line">        with open(path, &#39;a&#39;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            f.write(name + &#39;\n&#39;)</span><br><span class="line">            f.writelines(text)</span><br><span class="line">            f.write(&#39;\n\n&#39;)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    dl &#x3D; downloader()</span><br><span class="line">    dl.get_download_url()</span><br><span class="line">    print(&#39;《一年永恒》开始下载：&#39;)</span><br><span class="line">    for i in range(dl.nums):</span><br><span class="line">        dl.writer(dl.names[i], &#39;一念永恒.txt&#39;, dl.get_contents(dl.urls[i]))</span><br><span class="line">        sys.stdout.write(&quot;  已下载:%.3f%%&quot; %  float(i&#x2F;dl.nums) + &#39;\r&#39;)</span><br><span class="line">        sys.stdout.flush()</span><br><span class="line">    print(&#39;《一年永恒》下载完成&#39;)</span><br><span class="line">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687</span><br></pre></td></tr></table></figure>

<p>很简单的程序，单进程跑，没有开进程池。下载速度略慢，喝杯茶休息休息吧。代码运行效果如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20170928144348411?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<h2 id="2-优美壁纸下载"><a href="#2-优美壁纸下载" class="headerlink" title="2 优美壁纸下载"></a>2 优美壁纸下载</h2><p>###（1）实战背景</p>
<p>已经会爬取文字了，是不是感觉爬虫还是蛮好玩的呢？接下来，让我们进行一个进阶实战，了解一下反爬虫。</p>
<p>URL：<a href="https://unsplash.com/" target="_blank" rel="noopener">https://unsplash.com/</a></p>
<p><img src="https://img-blog.csdn.net/20170928144442435?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>看一看这些优美的壁纸，这个网站的名字叫做Unsplash，免费高清壁纸分享网是一个坚持每天分享高清的摄影图片的站点，每天更新一张高质量的图片素材，全是生活中的景象作品，清新的生活气息图片可以作为桌面壁纸也可以应用于各种需要的环境。</p>
<p>看到这么优美的图片，我的第一反应就是想收藏一些，作为知乎文章的题图再好不过了。每张图片我都很喜欢，批量下载吧，不多爬，就下载50张好了。</p>
<p>###（2）实战进阶</p>
<p>我们已经知道了每个html标签都有各自的功能。<code>&lt;a&gt;</code>标签存放一下超链接，图片存放在哪个标签里呢？html规定，图片统统给我放到<code>&lt;img&gt;</code>标签中！既然这样，我们截取就Unsplash网站中的一个<code>&lt;img&gt;</code>标签，分析一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;img alt&#x3D;&quot;Snow-capped mountain slopes under blue sky&quot; src&#x3D;&quot;https:&#x2F;&#x2F;images.unsplash.com&#x2F;photo-1428509774491-cfac96e12253?dpr&#x3D;1&amp;auto&#x3D;compress,format&amp;fit&#x3D;crop&amp;w&#x3D;360&amp;h&#x3D;240&amp;q&#x3D;80&amp;cs&#x3D;tinysrgb&amp;crop&#x3D;&quot; class&#x3D;&quot;cV68d&quot; style&#x3D;&quot;width: 220px; height: 147px;&quot;&gt;</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>可以看到，<code>&lt;img&gt;</code>标签有很多属性，有alt、src、class、style属性，其中src属性存放的就是我们需要的图片保存地址，我们根据这个地址就可以进行图片的下载。</p>
<p>那么，让我们先捋一捋这个过程：</p>
<ul>
<li>使用requeusts获取整个网页的HTML信息；</li>
<li>使用Beautiful Soup解析HTML信息，找到所有<code>&lt;img&gt;</code>标签，提取src属性，获取图片存放地址；</li>
<li>根据图片存放地址，下载图片。</li>
</ul>
<p>我们信心满满地按照这个思路爬取Unsplash试一试，编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">     target &#x3D; &#39;https:&#x2F;&#x2F;unsplash.com&#x2F;&#39;</span><br><span class="line">     req &#x3D; requests.get(url&#x3D;target)</span><br><span class="line">     print(req.text)</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>

<p>按照我们的设想，我们应该能找到很多<code>&lt;img&gt;</code>标签。但是我们发现，除了一些<code>&lt;script&gt;</code>标签和一些看不懂的代码之外，我们一无所获，一个<code>&lt;img&gt;</code>标签都没有！跟我们在网站审查元素的结果完全不一样，这是为什么？</p>
<p><img src="https://img-blog.csdn.net/20170928144659798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p><strong>答案就是，这个网站的所有图片都是动态加载的！</strong>网站有静态网站和动态网站之分，上一个实战爬取的网站是静态网站，而这个网站是动态网站，动态加载有一部分的目的就是为了反爬虫。</p>
<p><strong>对于什么是动态加载，你可以这样理解：我们知道化妆术学的好，贼厉害，可以改变一个人的容貌。相应的，动态加载用的好，也贼厉害，可以改变一个网站的容貌。</strong></p>
<p>动态网站使用动态加载常用的手段就是通过调用JavaScript来实现的。怎么实现JavaScript动态加载，我们不必深究，我们只要知道，动态加载的JavaScript脚本，就像化妆术需要用的化妆品，五花八门。有粉底、口红、睫毛膏等等，它们都有各自的用途。动态加载的JavaScript脚本也一样，一个动态加载的网站可能使用很多JavaScript脚本，我们只要找到负责动态加载图片的JavaScript脚本，不就找到我们需要的链接了吗？</p>
<p>对于初学者，我们不必看懂JavaScript执行的内容是什么，做了哪些事情，因为我们有强大的抓包工具，它自然会帮我们分析。这个强大的抓包工具就是Fiddler：</p>
<p>URL：<a href="http://www.telerik.com/fiddler" target="_blank" rel="noopener">http://www.telerik.com/fiddler</a></p>
<p>PS：也可以使用浏览器自带的Networks，但是我更推荐这个软件，因为它操作起来更高效。</p>
<p>安装方法很简单，傻瓜式安装，一直下一步即可，对于经常使用电脑的人来说，应该没有任何难度。</p>
<p>这个软件的使用方法也很简单，打开软件，然后用浏览器打开我们的目标网站，以Unsplash为例，抓包结果如下：</p>
<p><img src="https://img-blog.csdn.net/20170928144807582?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们可以看到，上图左侧红框处是我们的GET请求的地址，就是网站的URL，右下角是服务器返回的信息，我们可以看到，这些信息也是我们上一个程序获得的信息。这个不是我们需要的链接，我们继续往下看。</p>
<p><img src="https://img-blog.csdn.net/20170930125322590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我们发现上图所示的就是一个JavaScript请求，看右下侧服务器返回的信息是一个json格式的数据。这里面，就有我们需要的内容。我们局部放大看一下：</p>
<p><img src="https://img-blog.csdn.net/20170930125356095?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>这是Fiddler右侧的信息，上面是请求的Headers信息，包括这个Javascript的请求地 址：<a href="http://unsplash.com/napi/feeds/home，其他信息我们先不管，我们看看下面的内容。里面有很多图片的信息，包括图片的id，图片的大小，图片的链接，还有下一页的地址。这个脚本以json格式存储传输的数据，json格式是一种轻量级的数据交换格式，起到封装数据的作用，易于人阅读和编写，同时也易于机器解析和生成。这么多链接，可以看到图片的链接有很多，根据哪个链接下载图片呢？先别急，让我们继续分析：" target="_blank" rel="noopener">http://unsplash.com/napi/feeds/home，其他信息我们先不管，我们看看下面的内容。里面有很多图片的信息，包括图片的id，图片的大小，图片的链接，还有下一页的地址。这个脚本以json格式存储传输的数据，json格式是一种轻量级的数据交换格式，起到封装数据的作用，易于人阅读和编写，同时也易于机器解析和生成。这么多链接，可以看到图片的链接有很多，根据哪个链接下载图片呢？先别急，让我们继续分析：</a></p>
<p><img src="https://img-blog.csdn.net/20170930125434279?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>在这个网站，我们可以按这个按钮进行图片下载。我们抓包分下下这个动作，看看发送了哪些请求。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;unsplash.com&#x2F;photos&#x2F;1PrQ2mHW-Fo&#x2F;download?force&#x3D;true</span><br><span class="line">https:&#x2F;&#x2F;unsplash.com&#x2F;photos&#x2F;JX7nDtafBcU&#x2F;download?force&#x3D;true</span><br><span class="line">https:&#x2F;&#x2F;unsplash.com&#x2F;photos&#x2F;HCVbP3zqX4k&#x2F;download?force&#x3D;true</span><br><span class="line">123</span><br></pre></td></tr></table></figure>

<p>通过Fiddler抓包，我们发现，点击不同图片的下载按钮，GET请求的地址都是不同的。但是它们很有规律，就是中间有一段代码是不一样的，其他地方都一样。中间那段代码是不是很熟悉？没错，它就是我们之前抓包分析得到json数据中的照片的id。我们只要解析出每个照片的id，就可以获得图片下载的请求地址，然后根据这个请求地址，我们就可以下载图片了。那么，现在的首要任务就是解析json数据了。</p>
<p>json格式的数据也是分层的。可以看到next_page里存放的是下一页的请求地址，很显然Unsplash下一页的内容，也是动态加载的。在photos下面的id里，存放着图片的id，这个就是我们需要获得的图片id号。</p>
<p>怎么编程提取这些json数据呢？我们也是分步完成：</p>
<ul>
<li>获取整个json数据</li>
<li>解析json数据</li>
</ul>
<p>编写代码，尝试获取json数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;unsplash.com&#x2F;napi&#x2F;feeds&#x2F;home&#39;</span><br><span class="line">     req &#x3D; requests.get(url&#x3D;target) print(req.text)</span><br><span class="line">12345</span><br></pre></td></tr></table></figure>

<p>很遗憾，程序报错了，问题出在哪里？通过错误信息，我们可以看到SSL认证错误，SSL认证是指客户端到服务器端的认证。一个非常简单的解决这个认证错误的方法就是设置requests.get()方法的verify参数。这个参数默认设置为True，也就是执行认证。我们将其设置为False，绕过认证不就可以了？</p>
<p><img src="https://img-blog.csdn.net/20170930125540248?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>有想法就要尝试，编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;unsplash.com&#x2F;napi&#x2F;feeds&#x2F;home&#39;</span><br><span class="line">     req &#x3D; requests.get(url&#x3D;target, verify&#x3D;False)</span><br><span class="line">     print(req.text)</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>

<p>认证问题解决了，又有新问题了：</p>
<p><img src="https://img-blog.csdn.net/20170930125626169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>可以看到，我们GET请求又失败了，这是为什么？这个网站反爬虫的手段除了动态加载，还有一个反爬虫手段，那就是验证Request Headers。接下来，让我们分析下这个Requests Headers：</p>
<p><img src="https://img-blog.csdn.net/20170930125652596?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>我截取了Fiddler的抓包信息，可以看到Requests Headers里又很多参数，有Accept、Accept-Encoding、Accept-Language、DPR、User-Agent、Viewport-Width、accept-version、Referer、x-unsplash-client、authorization、Connection、Host。它们都是什么意思呢？</p>
<p>专业的解释能说的太多，我挑重点：</p>
<ul>
<li>User-Agent：这里面存放浏览器的信息。可以看到上图的参数值，它表示我是通过Windows的Chrome浏览器，访问的这个服务器。如果我们不设置这个参数，用Python程序直接发送GET请求，服务器接受到的User-Agent信息就会是一个包含python字样的User-Agent。如果后台设计者验证这个User-Agent参数是否合法，不让带Python字样的User-Agent访问，这样就起到了反爬虫的作用。这是一个最简单的，最常用的反爬虫手段。</li>
<li>Referer：这个参数也可以用于反爬虫，它表示这个请求是从哪发出的。可以看到我们通过浏览器访问网站，这个请求是从<a href="https://unsplash.com/，这个地址发出的。如果后台设计者，验证这个参数，对于不是从这个地址跳转过来的请求一律禁止访问，这样就也起到了反爬虫的作用。" target="_blank" rel="noopener">https://unsplash.com/，这个地址发出的。如果后台设计者，验证这个参数，对于不是从这个地址跳转过来的请求一律禁止访问，这样就也起到了反爬虫的作用。</a></li>
<li>authorization：这个参数是基于AAA模型中的身份验证信息允许访问一种资源的行为。在我们用浏览器访问的时候，服务器会为访问者分配这个用户ID。如果后台设计者，验证这个参数，对于没有用户ID的请求一律禁止访问，这样就又起到了反爬虫的作用。</li>
</ul>
<p>Unsplash是根据哪个参数反爬虫的呢？根据我的测试，是authorization。我们只要通过程序手动添加这个参数，然后再发送GET请求，就可以顺利访问了。怎么什么设置呢？还是requests.get()方法，我们只需要添加headers参数即可。编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;unsplash.com&#x2F;napi&#x2F;feeds&#x2F;home&#39;</span><br><span class="line">     headers &#x3D; &#123;&#39;authorization&#39;:&#39;your Client-ID&#39;&#125;</span><br><span class="line">     req &#x3D; requests.get(url&#x3D;target, headers&#x3D;headers, verify&#x3D;False)</span><br><span class="line">     print(req.text)</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>

<p>headers参数值是通过字典传入的。记得将上述代码中your Client-ID换成诸位自己抓包获得的信息。代码运行结果如下：</p>
<p><img src="https://img-blog.csdn.net/20170930125810442?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>皇天不负有心人，可以看到我们已经顺利获得json数据了，里面有next_page和照片的id。接下来就是解析json数据。根据我们之前分析可知，next_page放在了json数据的最外侧，照片的id放在了photos-&gt;id里。我们使用json.load()方法解析数据，编写代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests, json</span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">     target &#x3D; &#39;http:&#x2F;&#x2F;unsplash.com&#x2F;napi&#x2F;feeds&#x2F;home&#39;</span><br><span class="line">     headers &#x3D; &#123;&#39;authorization&#39;:&#39;your Client-ID&#39;&#125;</span><br><span class="line">     req &#x3D; requests.get(url&#x3D;target, headers&#x3D;headers, verify&#x3D;False)</span><br><span class="line">     html &#x3D; json.loads(req.text)</span><br><span class="line">     next_page &#x3D; html[&#39;next_page&#39;]</span><br><span class="line">     print(&#39;下一页地址:&#39;,next_page)</span><br><span class="line">     for each in html[&#39;photos&#39;]:</span><br><span class="line">          print(&#39;图片ID:&#39;,each[&#39;id&#39;])</span><br><span class="line">1234567891011</span><br></pre></td></tr></table></figure>

<p>解析json数据很简单，跟字典操作一样，就是字典套字典。json.load()里面的参数是原始的json格式的数据。程序运行结果如下：</p>
<p><img src="https://img-blog.csdn.net/20170930125850471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>图片的ID已经获得了，再通过字符串处理一下，就生成了我们需要的图片下载请求地址。根据这个地址，我们就可以下载图片了。下载方式，使用直接写入文件的方法。</p>
<p>###（3）整合代码</p>
<p>每次获取链接加一个1s延时，因为人在浏览页面的时候，翻页的动作不可能太快。我们要让我们的爬虫尽量友好一些。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:UTF-8 -*-</span><br><span class="line">import requests, json, time, sys</span><br><span class="line">from contextlib import closing</span><br><span class="line"></span><br><span class="line">class get_photos(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.photos_id &#x3D; []</span><br><span class="line">        self.download_server &#x3D; &#39;https:&#x2F;&#x2F;unsplash.com&#x2F;photos&#x2F;xxx&#x2F;download?force&#x3D;trues&#39;</span><br><span class="line">        self.target &#x3D; &#39;http:&#x2F;&#x2F;unsplash.com&#x2F;napi&#x2F;feeds&#x2F;home&#39;</span><br><span class="line">        self.headers &#x3D; &#123;&#39;authorization&#39;:&#39;Client-ID c94869b36aa272dd62dfaeefed769d4115fb3189a9d1ec88ed457207747be626&#39;&#125;</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    函数说明:获取图片ID</span><br><span class="line">    Parameters:</span><br><span class="line">        无</span><br><span class="line">    Returns:</span><br><span class="line">        无</span><br><span class="line">    Modify:</span><br><span class="line">        2017-09-13</span><br><span class="line">    &quot;&quot;&quot;   </span><br><span class="line">    def get_ids(self):</span><br><span class="line">        req &#x3D; requests.get(url&#x3D;self.target, headers&#x3D;self.headers, verify&#x3D;False)</span><br><span class="line">        html &#x3D; json.loads(req.text)</span><br><span class="line">        next_page &#x3D; html[&#39;next_page&#39;]</span><br><span class="line">        for each in html[&#39;photos&#39;]:</span><br><span class="line">            self.photos_id.append(each[&#39;id&#39;])</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        for i in range(5):</span><br><span class="line">            req &#x3D; requests.get(url&#x3D;next_page, headers&#x3D;self.headers, verify&#x3D;False)</span><br><span class="line">            html &#x3D; json.loads(req.text)</span><br><span class="line">            next_page &#x3D; html[&#39;next_page&#39;]</span><br><span class="line">            for each in html[&#39;photos&#39;]:</span><br><span class="line">                self.photos_id.append(each[&#39;id&#39;])</span><br><span class="line">            time.sleep(1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    函数说明:图片下载</span><br><span class="line">    Parameters:</span><br><span class="line">        无</span><br><span class="line">    Returns:</span><br><span class="line">        无</span><br><span class="line">    Modify:</span><br><span class="line">        2017-09-13</span><br><span class="line">    &quot;&quot;&quot;   </span><br><span class="line">    def download(self, photo_id, filename):</span><br><span class="line">        headers &#x3D; &#123;&#39;User-Agent&#39;:&#39;Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;61.0.3163.79 Safari&#x2F;537.36&#39;&#125;</span><br><span class="line">        target &#x3D; self.download_server.replace(&#39;xxx&#39;, photo_id)</span><br><span class="line">        with closing(requests.get(url&#x3D;target, stream&#x3D;True, verify &#x3D; False, headers &#x3D; self.headers)) as r:</span><br><span class="line">            with open(&#39;%d.jpg&#39; % filename, &#39;ab+&#39;) as f:</span><br><span class="line">                for chunk in r.iter_content(chunk_size &#x3D; 1024):</span><br><span class="line">                    if chunk:</span><br><span class="line">                        f.write(chunk)</span><br><span class="line">                        f.flush()</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    gp &#x3D; get_photos()</span><br><span class="line">    print(&#39;获取图片连接中:&#39;)</span><br><span class="line">    gp.get_ids()</span><br><span class="line">    print(&#39;图片下载中:&#39;)</span><br><span class="line">    for i in range(len(gp.photos_id)):</span><br><span class="line">        print(&#39;  正在下载第%d张图片&#39; % (i+1))</span><br><span class="line">        gp.download(gp.photos_id[i], (i+1))</span><br><span class="line">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364</span><br></pre></td></tr></table></figure>

<p>下载速度还行，有的图片下载慢是因为图片太大。可以看到右侧也打印了一些警报信息，这是因为我们没有进行SSL验证。</p>
<p><img src="https://img-blog.csdn.net/20170930130019403?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYzQwNjQ5NTc2Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>学会了爬取图片，简单的动态加载的网站也难不倒你了。赶快试试国内的一些图片网站吧！</p>
<h2 id="3-视频下载"><a href="#3-视频下载" class="headerlink" title="3 视频下载"></a>3 视频下载</h2><p>视频下载教程，请到这里查看：</p>
<p><a href="https://cuijiahua.com/blog/2017/10/spider_tutorial_1.html" target="_blank" rel="noopener">https://cuijiahua.com/blog/2017/10/spider_tutorial_1.html</a></p>
<h1 id="四-总结"><a href="#四-总结" class="headerlink" title="四 总结"></a>四 总结</h1><ul>
<li>本次Chat讲解的实战内容，均仅用于学习交流，请勿用于任何商业用途！</li>
<li>爬虫时效性低，同样的思路过了一个月，甚至一周可能无法使用，但是爬取思路都是如此，完全可以自行分析。</li>
<li>本次实战代码，均已上传我的Github，欢迎Follow、Star：<a href="https://github.com/Jack-Cherish/python-spider" target="_blank" rel="noopener">https://github.com/Jack-Cherish/python-spider</a></li>
<li>如有问题，请留言。如有错误，还望指正，谢谢！</li>
</ul>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2020/06/13/Python-%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/06/13/Python-%E7%88%AC%E8%99%AB%E5%A4%A7%E5%B0%8F%E9%A1%B9%E7%9B%AE%E9%9B%86%E5%90%88/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Python 爬虫大小项目集合
          
        </div>
      </a>
    
    
      <a href="/2020/06/13/Python-%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python 爬虫（一）</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        app_id: '9KjOP8UIrxMNeout6rbPNABM-gzGzoHsz',
        app_key: 'p8jYOMTugImVVUXFa52wB63J',
        path: window.location.pathname,
        notify: 'true',
        verify: 'true',
        avatar: 'mp',
        placeholder: '给我的文章加点评论吧~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>


      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        广外编程社
      </li>
      <li>
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://v1.cnzz.com/z_stat.php?id=1278929797&amp;web_id=1278929797'></script>
        
      </li>
    </ul>
  </div>


        <!-- 不蒜子统计 -->
        <span id="busuanzi_container_site_pv">
                本站总访问量<span id="busuanzi_value_site_pv"></span>次
        </span>
        <span class="post-meta-divider">|</span>
        <span id="busuanzi_container_site_uv" style='display:none'>
                本站访客数<span id="busuanzi_value_site_uv"></span>人
        </span>
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

</footer>

<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("08/4/2020 17:38:00");//在此处修改你的建站时间，格式：月/日/年 时:分:秒
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/logo.png" alt="广外编程社官网"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="https://gw-lazybones.github.io/2020/05/30/%EF%BC%88%E7%BD%AE%E9%A1%B6%EF%BC%89GZFLS%20LazyBones%20%E7%AE%80%E4%BB%8B" target="_blank" rel="noopener">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>就当交个团费吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/share.js"></script>


<script src="/js/lazyload.min.js"></script>


<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['广外编程社官网', '提供Python，C++；hexo等资料', '提供Python，C++；hexo等资料'],
      startDelay: 0,
      typeSpeed: 200,
      loop: true,
      backSpeed: 100,
      showCursor: true
    });
  } catch (err) {
  }

</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
    onClick: (e) => {
      $('.toc-link').removeClass('is-active-link');
      $(`a[href=${e.target.hash}]`).addClass('is-active-link');
      $(e.target.hash).scrollIntoView();
      return false;
    }
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/js/ayer.js"></script>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>



<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.css">
<script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.2/dist/jquery.fancybox.min.js"></script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="86"
        src="//music.163.com/outchain/player?type=2&id=22707008&auto=1&height=66"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!--单击显示文字-->
<script type="text/javascript" src="/js/click_show_text.js"></script>

<canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
<script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
<script type="text/javascript" src="/js/fireworks.js"></script>

<!--浏览器搞笑标题-->
<script type="text/javascript" src="/js/FunnyTitle.js"></script>

<!--动态线条背景-->
<script type="text/javascript"
color="220,220,220" opacity='1.4' zIndex="-4" count="182" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
</script>
